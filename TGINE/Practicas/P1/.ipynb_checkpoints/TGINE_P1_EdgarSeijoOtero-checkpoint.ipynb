{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTICA 1: Extracción de datos de una red social\n",
    "\n",
    "En esta práctica, crearemos un pequeño script que se encargue de recuperar información de Reddit, una red social muy popular en la que tiene cabida cualquier tema que nos interese.\n",
    "A través del wrapper para Python PRAW, utilizaremos el API de Reddit para extraer los datos de posts y comentarios, los almacenaremos en disco y trabajaremos con ellos para extraer valiosa información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, importaremos los módulos de Python que nos harán falta para poder realizar el script. Los módulos son los siguientes:\n",
    "\n",
    "+ praw: Reddit API wrapper para Python.\n",
    "+ ElementTree: Módulo que nos permitirá crear y leer XML.\n",
    "+ datetime: Módulo para el tratamiento de fechas\n",
    "+ CountVectorizer y TfidfVectorizer: Módulos de scikit-learn que utilizaremos para el posterior análisis de los datos extraídos.\n",
    "+ numpy: Módulo para trabajar fácilmente con arrays y matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importamos el wrapper praw para acceder al api de reddit\n",
    "import praw\n",
    "# Importamos el parser XML\n",
    "import xml.etree.ElementTree as ET\n",
    "# Importamos datetime para tratar fechas\n",
    "import datetime\n",
    "# Importamos el CountVectorizer y el TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# Importamos numpy para realizar operaciones con matrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso será definir los valores necesarios para realizar la conexión al API de Reddit. Además, crearemos variables con los valores de subreddit, el número de posts que queremos recuperar y el nombre del fichero donde los vamos a almacenar en disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definimos los valores necesarios para realizar la conexion al api\n",
    "CLIENT_ID = 'hYM-vevrJQH5zA'\n",
    "CLIENT_SECRET = 't5HlntQcK052abHi7CLja-2ep_U'\n",
    "USERNAME = 'eseijo'\n",
    "PASSWORD = 'TGINE2017'\n",
    "USER_AGENT = 'script:redditextractor:0.1 (by /u/eseijo)'\n",
    "\n",
    "# Subreddit a utilizar\n",
    "SUBREDDIT = 'personalfinance'\n",
    "# Numero de posts a extraer\n",
    "N = 300\n",
    "# Nombre de fichero donde se guardaran los datos extraidos de reddit\n",
    "FILENAME = 'redditextractor_data.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, encapsularemos en funciones alguna de las funcionalidades que necesitamos. Las funciones son las siguientes:\n",
    "\n",
    "+ get_new_content: Función que devuelve los últimos contenidos de un subreddit.\n",
    "+ get_hot_content: Función que devuelve los contenidos más populares del momento en un subreddit.\n",
    "+ get_XML_from_reddit_comment: Función que, dado un comentario de Reddit, crea un XML y lo añade a una lista.\n",
    "+ get_XML_from_reddit_post: Función que, dado un post de Reddit, crea una lista que contiene el post y cada uno de sus comentarios en XML.\n",
    "+ get_XML_content_as_list_from_file: Función que lee un XML de posts/comentarios de Reddit y devuelve una lista con sus contenidos. El contenido será el texto incluido en el tag content y, en caso de que este esté vacío, el del tag title. Si ninguno de los dos tags contiene información, el elemento se descarta.\n",
    "+ get_central_terms_from_corpus: Función que devuelve los n términos centrales de un corpus. Utiliza el TfidfVectorizer filtrando stop words y las palabras que aparecen en menos de 10 documentos.\n",
    "+ get_most_repeated_terms_from_corpus: Función que devuelve los n términos más repetidos de un corpus. Utiliza el CountVectorizer filtrando stop words y las palabras que aparecen en menos de 10 documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funcion que devuelve los ultimos contenidos de un subreddit\n",
    "def get_new_content(reddit, subreddit, topN):\n",
    "    subreddit = reddit.subreddit(subreddit)\n",
    "    return subreddit.new(limit = topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funcion que devuelve los contenidos mas populares del momento para un subreddit\n",
    "def get_hot_content(reddit, subreddit, topN):\n",
    "    subreddit = reddit.subreddit(subreddit)\n",
    "    return subreddit.hot(limit = topN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque el API de Reddit devuelve como máximo 100 posts por petición, el wrapper PRAW se encarga de realizar las peticiones necesarias espaciándolas debidamente para cumplir con las restricciones que impone el API. Por lo tanto, si se piden 1250 posts, se realizarán 13 llamadas al API, una cada dos segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funcion que parsea un comentario a XML y lo anade a una lista\n",
    "def get_XML_from_reddit_comment(entry, acc = []):\n",
    "    e = ET.Element('entry')\n",
    "    title = ET.SubElement(e, 'title')\n",
    "    content = ET.SubElement(e, 'content')\n",
    "    content.text = entry.body\n",
    "    date = ET.SubElement(e, 'date')\n",
    "    date.text = datetime.datetime.fromtimestamp(entry.created).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    type = ET.SubElement(e, 'type')\n",
    "    type.text = 'comment'\n",
    "    acc.append(e)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funcion que parsea un post y sus comentarios a XML y devuelve una lista\n",
    "def get_XML_from_reddit_post(entry):\n",
    "    acc = []\n",
    "    e = ET.Element('entry')\n",
    "    title = ET.SubElement(e, 'title')\n",
    "    title.text = entry.title\n",
    "    content = ET.SubElement(e, 'content')\n",
    "    content.text = entry.selftext\n",
    "    date = ET.SubElement(e, 'date')\n",
    "    date.text = datetime.datetime.fromtimestamp(entry.created).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    type = ET.SubElement(e, 'type')\n",
    "    type.text = 'post'\n",
    "    acc.append(e)\n",
    "    for comment in entry.comments:\n",
    "        if isinstance(comment, praw.models.MoreComments):\n",
    "            continue\n",
    "        get_XML_from_reddit_comment(comment, acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funcion que lee un XML de posts/comentarios de un fichero y devuelve una lista con sus contenidos\n",
    "def get_XML_content_as_list_from_file(filename):\n",
    "    parser = ET.XMLParser(encoding = 'utf-8')\n",
    "    root = ET.parse(filename, parser = parser).getroot()\n",
    "    result = []\n",
    "    for entry in root:\n",
    "        if entry.find('content').text != None:\n",
    "            result.append(entry.find('content').text)\n",
    "        elif entry.find('title').text != None:\n",
    "            result.append(entry.find('title').text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funcion que devuelve los n terminos centrales de un corpus\n",
    "def get_central_terms_from_corpus(corpus, n = 10):\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english', min_df = 10)\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "    acc_sum = np.sum(tfidf, axis = 0)\n",
    "    top_positions = np.argsort(-acc_sum).A1[:n]\n",
    "    terms = []\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for pos in top_positions:\n",
    "        terms.append(feature_names[pos])\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funcion que devuelve los n terminos mas repetidos de un corpus\n",
    "def get_most_repeated_terms_from_corpus(corpus, n = 100):\n",
    "    vectorizer = CountVectorizer(stop_words = 'english', min_df = 10)\n",
    "    count = vectorizer.fit_transform(corpus)\n",
    "    acc_sum = np.sum(count, axis = 0)\n",
    "    top_positions = np.argsort(-acc_sum).A1[:n]\n",
    "    terms = []\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for pos in top_positions:\n",
    "        terms.append(feature_names[pos])\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras la definición de las funciones, ya estamos en condiciones de ejecutar el código y realizar la extracción de datos para su análisis. Para ello, empezaremos realizando la conexión al API de Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 5.1.0 of praw is outdated. Version 5.2.0 was released Tuesday October 24, 2017.\n"
     ]
    }
   ],
   "source": [
    "# Realizar la conexion al api de reddit\n",
    "reddit = praw.Reddit(client_id = CLIENT_ID, client_secret = CLIENT_SECRET,\n",
    "                     user_agent = USER_AGENT, username = USERNAME, password = PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una lista vacía en la que meteremos los ids de los posts procesados por si alguna petición nos devuelve alguno que ya hemos tratado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postIds = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el elemento root del XML. EL XML resultante tendrá un aspecto como este:\n",
    "\n",
    "<pre>\n",
    "    &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n",
    "    &lt;entryList&gt;\n",
    "      &lt;entry&gt;\n",
    "        &lt;title&gt;What's your favorite 90s TV show?&lt;/title&gt;\n",
    "        &lt;content /&gt;\n",
    "        &lt;date&gt;2017-11-02 19:27:56&lt;/date&gt;\n",
    "        &lt;type&gt;post&lt;/type&gt;\n",
    "      &lt;/entry&gt;\n",
    "      &lt;entry&gt;\n",
    "        &lt;title /&gt;\n",
    "        &lt;content&gt;*Rugrats*.&lt;/content&gt;\n",
    "        &lt;date&gt;2017-11-02 19:28:25&lt;/date&gt;\n",
    "        &lt;type&gt;comment&lt;/type&gt;\n",
    "      &lt;/entry&gt;\n",
    "      &lt;entry&gt;\n",
    "        &lt;title&gt;What was the worst accident you have seen?&lt;/title&gt;\n",
    "        &lt;content /&gt;\n",
    "        &lt;date&gt;2017-11-02 19:27:27&lt;/date&gt;\n",
    "        &lt;type&gt;post&lt;/type&gt;\n",
    "      &lt;/entry&gt;\n",
    "      &lt;entry&gt;\n",
    "        &lt;title&gt;People of Reddit, what is your best \"Ay caramba\" moment?&lt;/title&gt;\n",
    "        &lt;content /&gt;\n",
    "        &lt;date&gt;2017-11-02 19:25:58&lt;/date&gt;\n",
    "        &lt;type&gt;post&lt;/type&gt;\n",
    "      &lt;/entry&gt;\n",
    "      [...]\n",
    "    &lt;/entryList&gt;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = ET.Element('entryList')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperamos el contenido más popular del subreddit e iteramos sobre él, creando en cada iteración una lista de XML que incluirá el post y sus comentarios y que añadiremos como hijos al elemento &lt;entryList&gt;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in get_hot_content(reddit, SUBREDDIT, N):\n",
    "    if post.id not in postIds:\n",
    "        e.extend(get_XML_from_reddit_post(post))\n",
    "        postIds.append(post.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de que se quisiese hacer con el contenido más reciente, el código sería similar y tan sólo habría que reemplazar la función get_hot_content por get_new_content. También se podrían usar ambas a la vez (o todas las que se necesiten) ya que al guardar los ids de los posts, evitamos introducir repetidos a nuestra colección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for post in get_new_content(reddit, subreddit, n):\n",
    "#     if post.id not in postIds:\n",
    "#         e.extend(get_XML_from_reddit_post(post))\n",
    "#         postIds.append(post.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras recuperar el contenido y haber formado el XML, lo escribimos en un fichero local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(FILENAME, 'w')\n",
    "print(ET.tostring(e).decode('utf-8'), file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la información debidamente normalizada y almacenada, procederemos a trabajar con ella.\n",
    "<br/>\n",
    "Empezaremos recuperando la información del XML y creando una lista con el contenido de cada post/comentario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recuperamos el contenido del fichero\n",
    "corpus = get_XML_content_as_list_from_file(FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos los 10 términos centrales de la colección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 terminos centrales para el subreddit personalfinance tras analizar 300 posts y sus comentarios (2074 documentos en total)\n",
      "['money', 'pay', 'credit', 'just', 'don', 'personalfinance', 'like', 'account', 'com', 'year']\n"
     ]
    }
   ],
   "source": [
    "# Mostramos los 10 terminos centrales\n",
    "print('10 terminos centrales para el subreddit {} tras analizar {} posts y sus comentarios ({} documentos en total)'.format(SUBREDDIT, N, len(corpus)))\n",
    "print(get_central_terms_from_corpus(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, mostramos los 100 términos más repetidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 terminos mas repetidos en el subreddit personalfinance tras analizar 300 posts y sus comentarios (2074 documentos en total)\n",
      "['money', 'pay', 'credit', 'just', 'don', 'like', 'year', 'personalfinance', 'account', 'insurance', 'time', 'com', 'month', 'need', 'www', 'make', 'wiki', 'debt', 'reddit', 'want', 'savings', 'http', 'car', 'work', 'card', 'years', 'job', 'good', 've', 'income', 'know', 'loan', 'new', 'use', 'fund', 'think', 'll', 'going', 'tax', 'plan', 'buy', 'way', 'months', 'really', 'loans', 'paying', 'free', 'better', '000', 'bank', 'company', 'library', 'got', 'people', 'save', 'house', 'things', 'home', 'start', 'paid', 'payment', 'right', 'sure', 'cash', 'life', 'cost', 'student', 'probably', 'rate', 'expenses', '401k', 'help', 'questions', 'taxes', 'check', 'emergency', '10', 'look', 'subreddit', 'advice', 'health', 'balance', 'live', 'spend', 'lot', 'best', 'looking', 'does', 'day', 'contact', 'able', 'school', 'ira', 'getting', 'automatically', 'kids', 'helpful', 'long', 'spending', 'week']\n"
     ]
    }
   ],
   "source": [
    "# Mostramos los 100 terminos mas repetidos\n",
    "print('100 terminos mas repetidos en el subreddit {} tras analizar {} posts y sus comentarios ({} documentos en total)'.format(SUBREDDIT, N, len(corpus)))\n",
    "print(get_most_repeated_terms_from_corpus(corpus))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
